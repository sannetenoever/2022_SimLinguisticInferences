Berkeley parser 
The Berkeley Neural Parser (BNP) (Kitaev & Klein, 2018; Kitaev, Cao & Klein, 2018) uses an encoder-decoder architecture to parse a sequence of words into its constituency tree. The encoder takes as input a sequence of words, and summarises it to a vector representation. This vector is then sequentially transformed by a decoder into a labelled tree. The BNP uses an encoder that has a self-attentive mechanism to construct its summary vector. Specifically, the encoder consists of two parts - a word based portion to generate a context aware representation, and a chart based portion to generate span scores (see Kitaev, Cao & Klein, 2018). We are only concerned with the way the encoder generates the representation, so we limit our attention to the former. The encoder takes as input a sequence of word tokens which are then embedded in a d-dimensional space which is learned during the training phase. Next, these embeddings are sequentially transformed by a stack of 8 identical layers, each of which consists of two sublayers - a multi headed attention layer followed by a feed forward layer. A single attention head takes as input a T x d matrix, with T input words, and each with d dimensions. The attention mechanism finds the likelihood of a word at position t attending to every other word in the sequence. The model uses multiple heads to aggregate attention from 8 remote regions in the sequence. The output of the attention layer is projected to lie in the d-dimensional space. This is then input into a Feed Forward layer. To summarise, the encoder has 8 stacked layers, each of which contains a multi-headed (with 8 heads) attention sublayer followed by a feed forward layer. The representation obtained by multiheaded attention in the lower layers is propagated to the higher representations in this manner. 
To simulate this model, we use the English pretrained neural parser  (https://github.com/nikitakit/self-attentive-parser). We passed inputs based on 6 different conditions - regular sentences (a-n-v-a), wordlist, noun phrase, verb phrase, a fixed ungrammatical pattern (v-n-n-a), incoherent semantic patterns (shuffling words in the a-n-v-a sentences, but not word position). In the regular sentence condition, we use the Ding et al., [1] stimuli as is, with sentences such as - "fat rats smell fear ...". In the noun phrase (and verb phrase) condition, every pair of words forms a noun (and verb) phrase as in - "fat rats big dogs ...", "smell fear cuts grass ..." respectively. In the wordlist condition, the words were randomly shuffled before presentation, like "big fat smell dogs ....". In the ungrammatical pattern condition, every four words had a particular syntactic pattern (verb-noun-noun-adjective) - "smell rats dogs big ...". In the incoherent semantics condition, semantically null but syntactically valid constructions of the kind found in the regular sentence condition were used, for example - "wet rugs smell grass ...". Each sample in these conditions consist of 52 words (13 * 4 sentences) or time steps. We aggregate results over 20 such repetitions for every condition. In each of these conditions, we collect activations from every feed-forward sub-layer of the 8 stacked layers. Therefore, the activations for each condition has a shape (20, 8, 52, 1024), with the last index denoting the word dimension. For each sample and sub-layer, we applied a Hanning taper and zero-padded the data (on each side with the same width as the data). Then we calculate the Fourier transform and averaged the frequency response across the repetition and word dimension per condition. For visualization we up-sampled the data to 8 Hz (instead the original 4 Hz) by inserting zeros across all dimensions for every other time step and repeated the Fourier analysis.
