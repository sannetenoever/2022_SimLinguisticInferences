{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using CUDA!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/Code/')\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "import trees\n",
    "import vocabulary\n",
    "import nkutil\n",
    "import parse_nk\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parser\n",
    "\n",
    "temp = torch.load(str(Path(os.getcwd())) + \"/Data/en_charlstm_dev.93.61.pt\")\n",
    "parser = parse_nk.NKChartParser.from_spec(temp['spec'], temp['state_dict'])\n",
    "if 'UNK' in parser.tag_vocab.indices:\n",
    "    dummy_tag = 'UNK'\n",
    "else:\n",
    "    dummy_tag = parser.tag_vocab.value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to get activations from intermediate layers\n",
    "\n",
    "def process_sequences_perlayer(seq_batch):\n",
    "    outputs_0= []\n",
    "    def hook_0(module, input, output):\n",
    "        outputs_0.append(output)\n",
    "    outputs_1 = []\n",
    "    def hook_1(module, input, output):\n",
    "        outputs_1.append(output)\n",
    "    outputs_2 = []\n",
    "    def hook_2(module, input, output):\n",
    "        outputs_2.append(output)\n",
    "    outputs_3 = []\n",
    "    def hook_3(module, input, output):\n",
    "        outputs_3.append(output)\n",
    "    outputs_4 = []\n",
    "    def hook_4(module, input, output):\n",
    "        outputs_4.append(output)\n",
    "    outputs_5 = []\n",
    "    def hook_5(module, input, output):\n",
    "        outputs_5.append(output)\n",
    "    outputs_6 = []\n",
    "    def hook_6(module, input, output):\n",
    "        outputs_6.append(output)\n",
    "    outputs_7 = []\n",
    "    def hook_7(module, input, output):\n",
    "        outputs_7.append(output)\n",
    "\n",
    "    parser.encoder.ff_0.relu.register_forward_hook(hook_0)\n",
    "    parser.encoder.ff_1.relu.register_forward_hook(hook_1)\n",
    "    parser.encoder.ff_2.relu.register_forward_hook(hook_2)\n",
    "    parser.encoder.ff_3.relu.register_forward_hook(hook_3)\n",
    "    parser.encoder.ff_4.relu.register_forward_hook(hook_4)\n",
    "    parser.encoder.ff_5.relu.register_forward_hook(hook_5)\n",
    "    parser.encoder.ff_6.relu.register_forward_hook(hook_6)\n",
    "    parser.encoder.ff_7.relu.register_forward_hook(hook_7)\n",
    "\n",
    "    out, _ = parser.parse_batch(seq_batch)\n",
    "    del _\n",
    "\n",
    "    temp_acts = np.array([outputs_0[0].numpy()[1:-1,:],\n",
    "                 outputs_1[0].numpy()[1:-1,:], \n",
    "                 outputs_2[0].numpy()[1:-1,:],\n",
    "                 outputs_3[0].numpy()[1:-1,:],\n",
    "                 outputs_4[0].numpy()[1:-1,:],\n",
    "                 outputs_5[0].numpy()[1:-1,:],\n",
    "                 outputs_6[0].numpy()[1:-1,:],\n",
    "                 outputs_7[0].numpy()[1:-1,:]])\n",
    "    return temp_acts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(SenType = 'sen'):\n",
    "    # possible sentypes: \n",
    "    # sen:          regular sentence\n",
    "    # wordlist:     wordlists\n",
    "    # nongrammar:   grammatically correct, nonsensical\n",
    "    # nounphrase:   noun phrases\n",
    "    # verbphrase:   verb phrases\n",
    "    # random:       adjective noun verb noun random\n",
    "    \n",
    "    # Load dictionaries\n",
    "    base = str(Path(os.getcwd()))\n",
    "    \n",
    "    with open(base + '/Data/Ding_grammatical.csv', 'r') as f:\n",
    "        all_sentences = [row for row in csv.reader(f)]\n",
    "    nouns, adjectives, verbs = [],[],[]\n",
    "    for sentence in all_sentences:\n",
    "        nouns.append(sentence[1])\n",
    "        nouns.append(sentence[3])\n",
    "        adjectives.append(sentence[0])\n",
    "        verbs.append(sentence[2])\n",
    "    nouns = list(set(nouns))\n",
    "    adjectives = list(set(adjectives))\n",
    "    verbs = list(set(verbs))\n",
    "    all_words = nouns + adjectives + verbs\n",
    "\n",
    "\n",
    "    N = 60        \n",
    "    if SenType == 'sen':\n",
    "        # Load words as lists\n",
    "        with open(base + '/Data/Ding_grammatical.csv', 'r') as f:\n",
    "            sentence_list = [row for row in csv.reader(f)]\n",
    "            \n",
    "    elif SenType == 'wordlist':\n",
    "        sentence_list = [random.sample(all_words,4) for i in range(N)]\n",
    "        \n",
    "    elif SenType == 'nongrammar':   \n",
    "        sentence_list = list()\n",
    "        for i in range(N):\n",
    "            w1 = random.sample(adjectives,1)[0]\n",
    "            w2 = random.sample(nouns,1)[0]\n",
    "            w3 = random.sample(verbs,1)[0]\n",
    "            w4 = random.sample(nouns,1)[0]\n",
    "            sentence_list.append([w1,w2,w3,w4])\n",
    "            \n",
    "    elif SenType == 'nounphrase':        \n",
    "        sentence_list = list()\n",
    "        for i in range(N):\n",
    "            w1 = random.sample(adjectives,1)[0]\n",
    "            w2 = random.sample(nouns,1)[0]\n",
    "            w3 = random.sample(adjectives,1)[0]\n",
    "            w4 = random.sample(nouns,1)[0]\n",
    "            sentence_list.append([w1,w2,w3,w4])\n",
    "            \n",
    "    elif SenType == 'verbphrase':\n",
    "        sentence_list = list()\n",
    "        for i in range(N):\n",
    "            w1 = random.sample(verbs,1)[0]\n",
    "            w2 = random.sample(nouns,1)[0]\n",
    "            w3 = random.sample(verbs,1)[0]\n",
    "            w4 = random.sample(nouns,1)[0]\n",
    "            sentence_list.append([w1,w2,w3,w4])\n",
    "            \n",
    "    elif SenType == 'random':\n",
    "        sentence_list = list()\n",
    "        for i in range(N):\n",
    "            w1 = random.sample(verbs,1)[0]\n",
    "            w2 = random.sample(nouns,1)[0]\n",
    "            w3 = random.sample(nouns,1)[0]\n",
    "            w4 = random.sample(adjectives,1)[0]\n",
    "            sentence_list.append([w1,w2,w3,w4])   \n",
    "    return sentence_list#, embedding_dict, one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_matrix(seq_type = 'sen'):\n",
    "    all_sequences = create_sentences(SenType=seq_type)\n",
    "    random.shuffle(all_sequences)\n",
    "    all_samples = []\n",
    "    for i in range(20):\n",
    "        current_sample = random.sample(all_sequences,13)\n",
    "        current_sample = [word for seq in current_sample for word in seq]\n",
    "        all_samples.append(current_sample)\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations for all conditions\n",
    "saveoutput = 0\n",
    "seq_types = ['sen','wordlist','nongrammar','nounphrase','verbphrase','random']\n",
    "\n",
    "for i,seq_type in enumerate(seq_types):\n",
    "    seq_input = gen_data_matrix(seq_type=seq_type)\n",
    "    mean_powers = [[] for j in range(8)] # number of feed forward layers considered\n",
    "    all_activations = []\n",
    "    for seq in seq_input:\n",
    "        subbatch_sentences = [[(dummy_tag, word) for word in seq]]\n",
    "        activations = process_sequences_perlayer(subbatch_sentences)\n",
    "        all_activations.append(activations)\n",
    "    if saveoutput:\n",
    "        np.save(str(Path(os.getcwd())) +  '/Data/'\n",
    "                 seq_type, np.array(all_activations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
